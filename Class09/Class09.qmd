---
title: "Class 09: Mini-Project"
author: "Hailey Wheeler (A13312713)"
format: pdf
---

```{r}
fna.data <-"WisconsinCancer.csv"
wisc.df <- read.csv(fna.data, row.names = 1)
wisc.df
```

```{r}
wisc.data <- wisc.df[,-1]
diagnosis <- as.factor(wisc.df$diagnosis)
```

Q1. How many observations are in this dataset?

569

```{r}
nrow(wisc.df)
dim(wisc.df)
```


Q2. How many of the observations have a malignant diagnosis?

212 

```{r}
sum(as.logical(diagnosis=="M"))
wisc.df$diagnosis
sum(wisc.df$diagnosis == "M")
table(wisc.df$diagnosis)
```


Q3. How many variables/features in the data are suffixed with _mean?

10

```{r}
grep("_mean$", colnames(wisc.df))
grep("_mean$", colnames(wisc.df), value=T)
```

#2. Principal Component Analysis 

```{r}
colMeans(wisc.data)
apply(wisc.data,2,sd)
```

```{r}
wisc.pr <-prcomp(wisc.data, scale =TRUE)
summary(wisc.pr)
```
Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

0.4427 - 44% of variance 


Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

You need three components. 


Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

You need seven components. 

Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

The plot is illegible and very difficult to understand. The points and data seem obscure. 


```{r}
biplot(wisc.pr)
plot(wisc.pr$x)
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis)
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis, xlab = "PC1", ylab = "PC2")
```

Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

The malignant and benign cohorts are more merged when comparing PC1 and PC3. 

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3], col=diagnosis, xlab = "PC1", ylab = "PC3")
```


```{r}
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis
library(ggplot2)
ggplot(df) +
  aes(PC1, PC2, col=diagnosis) +
  geom_point()
```

```{r}
# Variance of each Principal Component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Total Variance / Variance of each PC 
v <- summary(wisc.pr)
pve <- pr.var/sum(pr.var)

plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```
Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC.

-0.2608538

```{r}
wisc.pr$rotation[,1]
wisc.pr$rotation["concave.points_mean","PC1"]
```


3. Hierarchical Clustering 

Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

The height is 19.

```{r}
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(data.dist)
plot(wisc.hclust)
abline(h=19, col = "red", lty = 2)
?abline
```


```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
table(wisc.hclust.clusters, diagnosis)
```

Q11. OPTIONAL: Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10? How do you judge the quality of your result in each case?


Q12. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

I prefer "ward.D2" because variance is minimized so it is more comprehensive. 

```{r}
d.pc <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d.pc, method ="ward.D2")
plot(wisc.pr.hclust)
```


```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
plot(wisc.pr$x[,1:2], col=g)
```


```{r}
# install.packages("rgl")
# library(rgl)
# library(rgl)
# plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=grps)

## I tried to install/download 'rgl' but it won't process in R, preventing me from plotting it. 
```


```{r}
d.pc <- dist(wisc.pr$x[,1:7])
wisc.pr.hclust <- hclust(d.pc, method = "ward.D2")
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
plot(wisc.pr.hclust.clusters)
```

Q13. How well does the newly created model with four clusters separate out the two diagnoses?

The new model splits the two diagnoses very significantly/clearly.

Q14. How well do the hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

The hierarchical clustering models condensed the data from the 4 branches into 2 for a more comprehensive review. 

```{r}
table(diagnosis, grps)
```

```{r}
table(wisc.hclust.clusters,diagnosis)
```

```{r}
table(wisc.pr.hclust.clusters, diagnosis)
```

Q15. OPTIONAL: Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?


```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```


```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

Q16. Which of these new patients should we prioritize for follow up based on your results?

We should prioritize patient group 2. 
